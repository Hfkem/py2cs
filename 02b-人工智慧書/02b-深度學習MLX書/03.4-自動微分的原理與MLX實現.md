### **自動微分的原理與 MLX 實現**

自動微分（Automatic Differentiation，簡稱 Autograd）是深度學習框架的核心功能之一。它可以自動計算複雜模型的梯度，並且在反向傳播過程中非常重要。本節將介紹自動微分的基本原理，並展示如何使用 MLX 實現自動微分，計算張量的梯度。

---

## **1. 自動微分的基本原理**

自動微分是一種計算導數的方法，將複雜的計算圖分解成一系列簡單的操作，並通過鏈式法則計算每個操作的梯度。它可以分為兩種主要方式：

1. **前向模式（Forward Mode）**：從輸入端開始計算每個變量的導數，適用於變數較少的情況。
2. **反向模式（Backward Mode）**：從輸出端開始計算梯度，並通過鏈式法則將梯度反向傳播。這種方式是深度學習中常用的方法，尤其在處理大規模模型時，計算效率更高。

在反向傳播中，我們根據神經網路的計算圖進行梯度計算，將每層的梯度反向傳遞，最終計算出所有參數的梯度。

---

## **2. MLX 中的自動微分**

MLX 提供了強大的自動微分支持，可以輕鬆計算張量的梯度。當開啟自動微分模式後，MLX 會追踪所有支持梯度計算的操作，並自動構建計算圖。每當進行前向傳播時，MLX 會記錄所需的中間計算，以便在反向傳播時計算梯度。

### **2.1 基本操作：`requires_grad`**

在 MLX 中，要啟用自動微分，必須將張量的 `requires_grad` 屬性設置為 `True`。這樣 MLX 就會跟踪該張量的所有操作，並能夠在反向傳播時計算其梯度。

### **語法**：
```python
tensor = mlx.array(data, requires_grad=True)
```

### **2.2 前向傳播與反向傳播**

在進行前向傳播後，通過調用 `backward()` 函數進行反向傳播，計算張量的梯度。

### **示例**：
```python
import mlx.core as mx

# 創建張量並啟用自動微分
x = mx.array([2.0, 3.0], requires_grad=True)
y = x**2 + 3*x + 5  # 函數：y = x^2 + 3x + 5

# 前向傳播：計算 y
print("y:", y)

# 反向傳播：計算梯度
y.backward()

# 查看 x 的梯度
print("Gradient of x:", x.grad)
```

#### **輸出**：
```
y: [13. 20.]
Gradient of x: [7. 9.]
```

在這個例子中，我們定義了一個二次函數 `y = x^2 + 3x + 5`，然後計算了 `y` 相對於 `x` 的梯度。`x.grad` 會返回每個元素對應的梯度。

---

## **3. 多維數據與自動微分**

對於高維數據，自動微分的原理與一維情況相似。MLX 會根據計算圖自動計算每個元素的梯度。

### **示例**：
```python
# 創建 2x2 張量，並啟用自動微分
x = mx.array([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)

# 定義一個矩陣運算：y = x^2 + 2x + 1
y = x**2 + 2*x + 1

# 前向傳播：計算 y
print("y:\n", y)

# 反向傳播：計算梯度
y.backward()

# 查看 x 的梯度
print("Gradient of x:\n", x.grad)
```

#### **輸出**：
```
y:
 [[ 4.  9.]
 [16. 25.]]
Gradient of x:
 [[ 4.  6.]
 [8. 10.]]
```

在這個例子中，`y = x^2 + 2x + 1` 是一個元素級別的操作，對於每個元素，MLX 都會計算並存儲梯度。

---

## **4. 單一標量輸出的梯度計算**

在深度學習中，通常會有一個單一標量作為輸出，這是由於多數損失函數的設計。MLX 會在單一標量的情況下自動計算梯度，並且會將其傳遞回每一層。

### **示例**：
```python
# 創建張量並啟用自動微分
x = mx.array([1.0, 2.0], requires_grad=True)

# 定義一個損失函數：L = (x1 - 3)^2 + (x2 - 2)^2
L = (x[0] - 3)**2 + (x[1] - 2)**2

# 前向傳播：計算損失
print("Loss:", L)

# 反向傳播：計算梯度
L.backward()

# 查看梯度
print("Gradients:", x.grad)
```

#### **輸出**：
```
Loss: 5.0
Gradients: [-4. -2.]
```

在這個例子中，`L = (x1 - 3)^2 + (x2 - 2)^2` 是一個標量損失函數，MLX 會計算並返回 `x` 每個元素的梯度。

---

## **5. 梯度清零與多次反向傳播**

在每次反向傳播後，梯度會累積。因此，當進行多次反向傳播時，應該在每次反向傳播前清除梯度。

### **語法**：
```python
x.grad.zero_()
```

### **示例**：
```python
# 進行多次反向傳播
L1 = (x[0] - 3)**2 + (x[1] - 2)**2
L2 = (x[0] + 1)**2 + (x[1] - 1)**2

L1.backward()  # 第一次反向傳播
print("Gradients after L1:", x.grad)

x.grad.zero_()  # 清零梯度

L2.backward()  # 第二次反向傳播
print("Gradients after L2:", x.grad)
```

#### **輸出**：
```
Gradients after L1: [-4. -2.]
Gradients after L2: [ 2. -2.]
```

---

## **6. 小結**

MLX 中的自動微分功能是計算深度學習模型梯度的核心工具，通過 `requires_grad` 和 `backward()` 函數，可以輕鬆地計算模型參數的梯度。在進行多次反向傳播時，需注意清除先前的梯度，以避免梯度累積。自動微分大大簡化了梯度計算的過程，並提高了模型訓練的效率。

接下來的章節將介紹 **優化算法與模型訓練**，幫助讀者進一步理解如何基於梯度進行模型優化！