### Transformer 模型的結構

Transformer 模型是由 Vaswani 等人在 2017 年提出的，並迅速成為深度學習領域中的重要架構，尤其在自然語言處理（NLP）領域取得了革命性的成果。Transformer 的核心優勢在於其完全基於自注意力（Self-Attention）機制，而不是傳統的卷積或循環神經網絡。這使得 Transformer 可以高效地處理長序列的依賴關係，並且並行處理所有位置的數據，極大地提高了訓練速度。

Transformer 的結構主要包括兩個部分：
- **編碼器（Encoder）**
- **解碼器（Decoder）**

這些組件在許多基於 Transformer 的架構（如 BERT、GPT）中會有不同的變體，但基本結構通常是相似的。

#### 1. **Transformer 編碼器（Encoder）**

編碼器的主要任務是將輸入序列映射到一個高維的表示空間。編碼器由一個堆疊的多層結構組成，每層由以下兩個主要組件構成：

1. **多頭自注意力層（Multi-Head Self-Attention Layer）**：
   - 自注意力機制允許每個位置的輸入向量根據所有其他位置的輸入向量進行加權求和，從而捕捉序列中元素之間的依賴關係。
   - 在多頭自注意力中，會有多個注意力頭（head），每個頭學習不同的關聯模式，然後將所有頭的輸出拼接起來。

2. **前向傳播層（Feedforward Layer）**：
   - 每個編碼器層都包括一個位置獨立的全連接前向傳播網絡。這通常是兩個線性變換層，並且包含激活函數（如 ReLU）。

每個編碼器層還有以下兩個關鍵組件：
- **殘差連接（Residual Connection）**：將每層的輸入與輸出進行相加，有助於減少梯度消失的問題。
- **層歸一化（Layer Normalization）**：對每層的輸出進行歸一化，有助於穩定訓練。

總結來說，編碼器的每一層結構如下：
\[
\text{Encoder Layer} = \text{LayerNorm}(\text{MultiHeadAttention}(Q, K, V) + \text{Input}) \rightarrow \text{LayerNorm}(\text{FeedForward}(\text{Input}) + \text{Output})
\]

#### 2. **Transformer 解碼器（Decoder）**

解碼器的主要任務是生成模型的輸出，這通常是目標序列。在自回歸的情況下，解碼器逐步生成輸出，直到生成完整的序列。解碼器的結構與編碼器類似，但有兩個主要區別：

1. **Masked 多頭自注意力層（Masked Multi-Head Self-Attention）**：
   - 解碼器中的第一個自注意力層是 **masked**，這意味著它僅能關注序列中當前位置之前的元素，從而防止未來的信息影響當前的預測。
   - 這樣的設計是為了在自回歸過程中避免信息泄露。

2. **編碼器-解碼器注意力層（Encoder-Decoder Attention）**：
   - 解碼器還有一個額外的注意力層，它將解碼器的查詢與編碼器的鍵和值進行交互，從而將編碼器生成的上下文信息與解碼器的當前輸入相結合。這使得解碼器能夠根據編碼器的輸出來生成輸出序列。

解碼器的每一層結構如下：
\[
\text{Decoder Layer} = \text{LayerNorm}(\text{Masked MultiHeadAttention}(Q, K, V) + \text{Input}) \rightarrow \text{LayerNorm}(\text{Encoder-Decoder Attention}(Q, K, V) + \text{Input}) \rightarrow \text{LayerNorm}(\text{FeedForward}(\text{Input}) + \text{Output})
\]

#### 3. **Transformer 的整體結構**

一個典型的 Transformer 模型包含多層編碼器和解碼器。在機器翻譯等任務中，編碼器負責處理源語言的序列，解碼器負責生成目標語言的序列。最終，解碼器的輸出會經過一個線性層，並通過 softmax 層生成每個詞的概率分佈。

**Transformer 的結構圖示：**

```
           +-------------+     +-----------------+      +------------------+
Input ---> | Encoder 1   |---->| Encoder 2       |----> | ...              |-----> Encoded Output
           +-------------+     +-----------------+      +------------------+
           
           +-------------+     +-----------------+      +------------------+
Input ---> | Decoder 1   |---->| Decoder 2       |----> | ...              |-----> Final Output
           +-------------+     +-----------------+      +------------------+
```

#### 4. **位置編碼（Positional Encoding）**

由於 Transformer 沒有使用循環或卷積結構，沒有內建的順序感知能力。因此，需要對輸入序列中的每個元素添加一個位置編碼。這些位置編碼是根據每個元素在序列中的位置進行設計的，通常會使用正弦和餘弦函數來生成：

\[
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})
\]
\[
PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})
\]

這些位置編碼會加到輸入序列的嵌入表示中，使得模型能夠學習到序列中元素的相對和絕對位置。

#### 5. **總結**

Transformer 模型是一個強大的序列處理架構，具有編碼器和解碼器結構，並且通過自注意力機制來捕捉序列中各個位置之間的依賴關係。其結構包括多頭自注意力層、前向傳播層、以及位置編碼等組件，這使得它在許多自然語言處理和序列生成任務中取得了顯著的成果。