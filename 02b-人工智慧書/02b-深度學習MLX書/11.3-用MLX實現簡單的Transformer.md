### 用 MLX 實現簡單的 Transformer

下面將介紹如何使用 MLX 實現一個簡單的 Transformer 模型。這將涵蓋 Transformer 的基本結構，包括編碼器、解碼器、自注意力機制和多頭注意力層。

#### 1. **準備工作**

首先，確保已經安裝了 MLX 和相關的 Python 庫：

```bash
pip install mlx numpy
```

接著，導入所需的庫：

```python
import mlx
import numpy as np
```

#### 2. **位置編碼**

在 Transformer 中，為了讓模型感知序列中的位置信息，我們需要為輸入序列添加位置編碼。這裡我們使用正弦和餘弦函數來生成位置編碼：

```python
def positional_encoding(seq_len, d_model):
    position = np.arange(seq_len)[:, np.newaxis]
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    pe = np.zeros((seq_len, d_model))
    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)
    return mlx.tensor(pe, dtype=mlx.float32)

# 示例：序列長度為10，特徵維度為512
pe = positional_encoding(10, 512)
print(pe.shape)
```

#### 3. **自注意力層（Self-Attention Layer）**

自注意力層是 Transformer 的核心組件，允許模型根據輸入序列中的不同位置計算加權和。這裡我們實現一個基本的自注意力層：

```python
class SelfAttention(mlx.nn.Module):
    def __init__(self, d_model, num_heads):
        super(SelfAttention, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.depth = d_model // num_heads

        self.query = mlx.nn.Linear(d_model, d_model)
        self.key = mlx.nn.Linear(d_model, d_model)
        self.value = mlx.nn.Linear(d_model, d_model)

        self.dense = mlx.nn.Linear(d_model, d_model)

    def split_heads(self, x):
        batch_size = x.shape[0]
        x = x.reshape(batch_size, -1, self.num_heads, self.depth)
        return x.transpose(1, 2)

    def forward(self, x):
        batch_size = x.shape[0]

        query = self.split_heads(self.query(x))
        key = self.split_heads(self.key(x))
        value = self.split_heads(self.value(x))

        attention_logits = mlx.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.depth)
        attention_weights = mlx.nn.functional.softmax(attention_logits, axis=-1)

        attention_output = mlx.matmul(attention_weights, value)
        attention_output = attention_output.transpose(1, 2).reshape(batch_size, -1, self.d_model)

        output = self.dense(attention_output)
        return output
```

#### 4. **前向傳播層（Feed-Forward Layer）**

每個 Transformer 層還包括一個前向傳播層，它通常是由兩個線性層和一個激活函數（如 ReLU）組成：

```python
class FeedForward(mlx.nn.Module):
    def __init__(self, d_model, ff_dim):
        super(FeedForward, self).__init__()
        self.dense1 = mlx.nn.Linear(d_model, ff_dim)
        self.dense2 = mlx.nn.Linear(ff_dim, d_model)

    def forward(self, x):
        x = mlx.nn.functional.relu(self.dense1(x))
        x = self.dense2(x)
        return x
```

#### 5. **Transformer 編碼器層（Encoder Layer）**

編碼器層結合了自注意力層和前向傳播層，並且使用殘差連接和層歸一化：

```python
class EncoderLayer(mlx.nn.Module):
    def __init__(self, d_model, num_heads, ff_dim):
        super(EncoderLayer, self).__init__()
        self.self_attention = SelfAttention(d_model, num_heads)
        self.feed_forward = FeedForward(d_model, ff_dim)

        self.layer_norm1 = mlx.nn.LayerNorm(d_model)
        self.layer_norm2 = mlx.nn.LayerNorm(d_model)

    def forward(self, x):
        # Self-Attention
        attention_output = self.self_attention(x)
        x = self.layer_norm1(x + attention_output)

        # Feedforward
        ff_output = self.feed_forward(x)
        x = self.layer_norm2(x + ff_output)

        return x
```

#### 6. **Transformer 解碼器層（Decoder Layer）**

解碼器層在編碼器層的基礎上增加了編碼器-解碼器注意力層：

```python
class DecoderLayer(mlx.nn.Module):
    def __init__(self, d_model, num_heads, ff_dim):
        super(DecoderLayer, self).__init__()
        self.self_attention = SelfAttention(d_model, num_heads)
        self.encoder_decoder_attention = SelfAttention(d_model, num_heads)  # 編碼器-解碼器注意力
        self.feed_forward = FeedForward(d_model, ff_dim)

        self.layer_norm1 = mlx.nn.LayerNorm(d_model)
        self.layer_norm2 = mlx.nn.LayerNorm(d_model)
        self.layer_norm3 = mlx.nn.LayerNorm(d_model)

    def forward(self, x, encoder_output):
        # Self-Attention
        attention_output = self.self_attention(x)
        x = self.layer_norm1(x + attention_output)

        # Encoder-Decoder Attention
        attention_output = self.encoder_decoder_attention(x, encoder_output)
        x = self.layer_norm2(x + attention_output)

        # Feedforward
        ff_output = self.feed_forward(x)
        x = self.layer_norm3(x + ff_output)

        return x
```

#### 7. **Transformer 模型**

將所有組件結合成一個簡單的 Transformer 模型，這個模型將包含編碼器和解碼器：

```python
class Transformer(mlx.nn.Module):
    def __init__(self, d_model, num_heads, ff_dim, num_encoder_layers, num_decoder_layers):
        super(Transformer, self).__init__()
        self.encoder_layers = mlx.nn.ModuleList([EncoderLayer(d_model, num_heads, ff_dim) for _ in range(num_encoder_layers)])
        self.decoder_layers = mlx.nn.ModuleList([DecoderLayer(d_model, num_heads, ff_dim) for _ in range(num_decoder_layers)])
        self.dense = mlx.nn.Linear(d_model, 1)  # 假設最終輸出是單一預測

    def forward(self, x):
        # 編碼器處理
        for layer in self.encoder_layers:
            x = layer(x)

        # 解碼器處理
        for layer in self.decoder_layers:
            x = layer(x, x)  # 假設編碼器輸出作為解碼器的輸入

        # 最終輸出
        output = self.dense(x)
        return output
```

#### 8. **模型訓練與測試**

現在我們有了一個簡單的 Transformer 模型，可以使用訓練數據進行訓練。訓練過程與其他模型類似，使用優化器和損失函數來更新模型參數。

```python
# 初始化 Transformer 模型
model = Transformer(d_model=512, num_heads=8, ff_dim=2048, num_encoder_layers=6, num_decoder_layers=6)

# 定義優化器和損失函數
optimizer = mlx.optim.Adam(model.parameters(), lr=0.001)
criterion = mlx.nn.MSELoss()

# 假設 x 是輸入數據，y 是目標輸出
x = mlx.tensor(np.random.randn(32, 10, 512), dtype=mlx.float32)  # 示例批次大小32，序列長度10，特徵維度512
y = mlx.tensor(np.random.randn(32, 10, 1), dtype=mlx.float32)

# 訓練循環
for epoch in range(100):
    optimizer.zero_grad()
    output = model(x)
    loss = criterion(output, y)
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")
```

#### 9. **總結**

這是一個基於 MLX 實現的簡單 Transformer 模型。該模型包括自注意力機制、前向傳播層、多層編碼器和解碼器，並且使用了位置編碼來捕捉序列中的位置信息。這樣的模型可以進行序列到序列的任務，如機器翻譯、文本生成等。