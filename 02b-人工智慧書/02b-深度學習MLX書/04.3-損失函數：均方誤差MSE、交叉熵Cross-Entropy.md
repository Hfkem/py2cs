### **損失函數：均方誤差（MSE）、交叉熵（Cross-Entropy）**

損失函數是機器學習和深度學習中的核心概念之一。它用來衡量模型預測結果與真實標籤之間的誤差，並且指導模型在訓練過程中如何調整權重以最小化這些誤差。不同的問題類型和模型結構會使用不同的損失函數，下面將介紹兩個常見的損失函數：均方誤差（MSE）和交叉熵（Cross-Entropy）。

---

## **1. 均方誤差（MSE, Mean Squared Error）**

均方誤差（MSE）是一種常用的回歸損失函數，通常用於測量連續數據的預測誤差。MSE 計算的是預測值和實際值之間差異的平方的平均值。它的數學公式為：

\[
MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y_i})^2
\]

其中：
- \( y_i \) 是實際值（標籤）。
- \( \hat{y_i} \) 是模型的預測值。
- \( N \) 是樣本數量。

### **MSE 的特點**：
- **簡單且直觀**：MSE 計算簡單，對於回歸問題，當模型的預測值與實際值接近時，MSE 的值會較小，反之則會較大。
- **敏感於異常值**：由於 MSE 是基於差值的平方，它對於極端異常值非常敏感，這意味著大誤差對最終損失值的影響會被放大。

### **MSE 的缺點**：
- **對異常值敏感**：如果數據中存在極端的異常值，這些異常值可能會顯著提高損失函數的值，導致模型訓練過程受到影響。

### **適用場景**：
MSE 通常用於回歸問題，如預測房價、股票價格、溫度等連續數值的問題。

### **MLX 實現範例**：

在 MLX 中，我們可以使用 `mlx.loss.MSELoss` 來實現均方誤差損失函數。以下是如何使用 MSE 損失函數的範例：

```python
import mlx.core as mx
import mlx.loss as loss

# 假設預測值與真實值
predicted = mx.array([2.5, 0.5, 2.1, 1.9])
actual = mx.array([3.0, 0.0, 2.0, 2.0])

# 計算 MSE 損失
mse_loss = loss.MSELoss()
loss_value = mse_loss(predicted, actual)

print("Mean Squared Error Loss:", loss_value)
```

---

## **2. 交叉熵（Cross-Entropy）**

交叉熵是一個常用於分類問題的損失函數，尤其在二分類和多分類問題中應用廣泛。它衡量的是預測分佈與真實分佈之間的差異。在機器學習中，交叉熵損失通常與 softmax 函數一起使用，用來處理多類別分類問題。

交叉熵的數學公式如下：

### **二分類交叉熵（Binary Cross-Entropy）**：

對於二分類問題，交叉熵損失函數可以寫作：

\[
CE = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y_i}) + (1 - y_i) \log(1 - \hat{y_i}) \right]
\]

其中：
- \( y_i \) 是真實標籤（0 或 1）。
- \( \hat{y_i} \) 是預測的概率（模型的輸出）。

### **多分類交叉熵（Categorical Cross-Entropy）**：

對於多分類問題，交叉熵的公式為：

\[
CE = -\sum_{i=1}^{N} y_i \log(\hat{y_i})
\]

其中 \( y_i \) 是真實標籤的 one-hot 編碼向量，\( \hat{y_i} \) 是對應類別的預測概率。

### **交叉熵的特點**：
- **適用於分類問題**：交叉熵損失函數通常用於處理分類問題，特別是當模型的輸出為概率分佈時。
- **計算易於實現**：計算交叉熵可以很容易地通過 softmax 輸出進行處理，因此適用於深度學習中的多分類問題。
- **數值穩定性**：交叉熵的計算中涉及到對數運算，因此需要注意數值穩定性問題，避免出現對零取對數的情況。

### **適用場景**：
交叉熵在多分類和二分類問題中非常常見，尤其在神經網絡中，通常與 softmax 層搭配使用來進行分類任務。

### **MLX 實現範例**：

在 MLX 中，我們可以使用 `mlx.loss.CrossEntropyLoss` 來實現交叉熵損失函數。以下是如何使用交叉熵損失函數的範例：

```python
import mlx.core as mx
import mlx.loss as loss

# 假設預測的 logits 和真實的標籤
logits = mx.array([[1.2, 0.9, 0.1], [0.4, 1.5, 1.1]])  # 假設有兩個樣本，三個類別的 logits
labels = mx.array([0, 2])  # 真實的標籤

# 計算交叉熵損失
cross_entropy_loss = loss.CrossEntropyLoss()
loss_value = cross_entropy_loss(logits, labels)

print("Cross Entropy Loss:", loss_value)
```

---

## **小結**

- **均方誤差（MSE）**：適用於回歸問題，計算預測值和實際值之間的平方差，對異常值非常敏感。
- **交叉熵（Cross-Entropy）**：適用於分類問題，衡量預測概率分佈與真實標籤分佈之間的差異，廣泛應用於二分類和多分類任務。

在 MLX 中，這兩種損失函數都可以輕鬆實現，並且可以根據不同的問題選擇合適的損失函數來訓練模型。