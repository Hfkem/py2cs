### **實現簡單的二分類問題**

在這一節中，我們將使用 MLX 框架來實現一個簡單的二分類問題。二分類問題是指將數據樣本分為兩個類別的問題。這裡我們將使用一個簡單的神經網絡來解決二分類問題，並演示如何訓練模型以進行預測。

---

### **1. 問題背景**

假設我們有一組數據集，每個樣本都有一些特徵（例如，10 維特徵），並且每個樣本都有一個二元標籤（`0` 或 `1`），代表兩個類別。

目標是訓練一個模型，使其能夠根據輸入的特徵預測該樣本屬於哪一類（`0` 或 `1`）。

---

### **2. 數據準備**

我們先生成一些隨機的數據來作為示例。這裡我們使用 `mlx.array` 創建一些隨機數據來模擬特徵和標籤。

```python
import mlx.core as mx
import numpy as np

# 隨機生成數據：假設有 100 個樣本，每個樣本有 10 維特徵
inputs = mx.array(np.random.randn(100, 10))  # 100 個樣本，10 維特徵
# 隨機生成標籤：每個樣本的標籤是 0 或 1
targets = mx.array(np.random.randint(0, 2, size=(100, 1)))  # 100 個標籤，0 或 1
```

在這個例子中，`inputs` 是一個大小為 `(100, 10)` 的張量，表示 100 個樣本，每個樣本有 10 維特徵。`targets` 是一個大小為 `(100, 1)` 的張量，包含每個樣本的二元標籤（`0` 或 `1`）。

---

### **3. 定義簡單的二分類神經網絡**

我們將構建一個簡單的多層感知器（MLP）來解決這個二分類問題。這個模型包含兩層：
1. 一個線性層，將輸入的 10 維特徵轉換為 50 維。
2. 一個線性層，將 50 維轉換為 1 維，並使用 Sigmoid 激活函數輸出概率。

#### **3.1 定義模型**

```python
import mlx.nn as nn

class SimpleBinaryNN(nn.Module):
    def __init__(self):
        super(SimpleBinaryNN, self).__init__()
        # 定義第一層：線性層 (10 -> 50)
        self.fc1 = nn.Linear(10, 50)
        # 定義第二層：線性層 (50 -> 1)
        self.fc2 = nn.Linear(50, 1)
        # 定義激活函數：ReLU
        self.relu = nn.ReLU()
        # 定義最終激活函數：Sigmoid（用於二分類）
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # 前向傳播：通過第一層 -> 激活 -> 通過第二層 -> Sigmoid 激活
        x = self.relu(self.fc1(x))  # ReLU 激活
        x = self.fc2(x)  # 第二層線性變換
        x = self.sigmoid(x)  # 最終激活函數，將輸出限制在 [0, 1] 範圍內
        return x
```

這個模型包含兩個線性層，並且在最後一層使用 `Sigmoid` 激活函數，這是典型的二分類問題的做法，因為 `Sigmoid` 可以將輸出轉換為 [0, 1] 範圍內的概率值。

---

### **4. 訓練模型**

接下來，我們定義損失函數和優化器，並實現訓練循環。

#### **4.1 損失函數與優化器**

對於二分類問題，通常使用 **二元交叉熵損失**（Binary Cross-Entropy Loss），這是衡量預測概率與真實標籤之間差距的常用方法。

```python
# 定義損失函數：二元交叉熵損失
loss_fn = nn.BCELoss()  # BCELoss 針對二分類問題

# 定義優化器：Adam 優化器
optimizer = nn.optim.Adam(model.parameters(), lr=0.001)
```

- **BCELoss** 是專門為二分類問題設計的損失函數，適用於輸出經過 `Sigmoid` 激活的情況。
- **Adam 優化器** 是一種自適應學習率優化器，能夠有效地更新模型的權重。

#### **4.2 訓練循環**

```python
# 創建模型實例
model = SimpleBinaryNN()

# 訓練循環的迭代次數
epochs = 1000

# 訓練循環
for epoch in range(epochs):
    optimizer.zero_grad()  # 清除上一輪的梯度
    outputs = model(inputs)  # 前向傳播
    loss = loss_fn(outputs, targets)  # 計算損失
    loss.backward()  # 反向傳播，計算梯度
    optimizer.step()  # 更新參數

    # 每 100 次迭代輸出一次損失
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")
```

在每一輪訓練中：
1. 我們先將梯度清零。
2. 將輸入 `inputs` 傳遞給模型，計算輸出。
3. 計算模型預測和真實標籤之間的損失。
4. 進行反向傳播計算梯度。
5. 使用優化器更新模型參數。

#### **4.3 評估模型**

訓練結束後，可以使用模型來對新數據進行預測：

```python
# 用模型進行預測
with mx.no_grad():  # 評估模式下不需要計算梯度
    predictions = model(inputs)
    predictions = (predictions > 0.5).float()  # 將概率轉換為 0 或 1
    print("預測結果:", predictions)
```

在這裡，我們使用 `predictions > 0.5` 將模型的預測結果轉換為二元標籤（`0` 或 `1`）。這是因為 Sigmoid 輸出的概率值範圍在 [0, 1] 之間，`0.5` 是一個常見的閾值。

---

### **5. 小結**

在這個範例中，我們使用 MLX 框架實現了一個簡單的二分類問題。主要步驟包括：
1. 定義數據集：創建隨機的特徵和標籤。
2. 定義模型：創建包含兩層的簡單神經網絡，並使用 `Sigmoid` 激活函數進行二分類。
3. 訓練模型：使用二元交叉熵損失和 Adam 優化器進行訓練。
4. 評估模型：使用訓練好的模型進行預測，並將結果轉換為標籤。

這樣，我們就能夠訓練一個簡單的神經網絡來解決二分類問題。