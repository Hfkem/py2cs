### **使用 MLX 實現文本預測案例**

在這個例子中，我們將使用 MLX 來實現一個簡單的文本預測模型，基於一個字符級的循環神經網絡（RNN）來進行字符級預測。這將包括數據處理、模型構建、訓練和測試過程。

假設我們有一個文本數據集，我們將基於這些文本預測下一個字符。

---

### **1. 數據準備**

首先，我們需要準備數據。這裡我們將使用一個簡單的文本文件，並對文本進行字符級處理。每次模型將基於一段輸入文本預測下一個字符。

```python
import numpy as np
import mlx

# 載入文本數據集
with open('shakespeare.txt', 'r') as file:
    text = file.read().lower()

# 創建字符到索引的映射
chars = sorted(list(set(text)))
char_to_idx = {char: idx for idx, char in enumerate(chars)}
idx_to_char = {idx: char for idx, char in enumerate(chars)}

# 將文本轉換為數字序列
text_as_int = np.array([char_to_idx[c] for c in text])
```

---

### **2. 創建數據生成器**

我們需要創建一個生成器，將文本切割成訓練樣本。每個樣本包括一段固定長度的字符序列，以及對應的下一個字符作為標籤。

```python
# 設定序列長度
seq_length = 100

def generate_batch(data, seq_length):
    input_batch = []
    target_batch = []
    for i in range(0, len(data) - seq_length, seq_length):
        input_batch.append(data[i:i+seq_length])
        target_batch.append(data[i+1:i+seq_length+1])
    return np.array(input_batch), np.array(target_batch)

# 生成訓練批次
input_batch, target_batch = generate_batch(text_as_int, seq_length)
```

---

### **3. 構建模型**

現在，我們將使用 `mlx.nn` 模組來構建一個簡單的 RNN 模型。這個模型將包含一個嵌入層、一個 RNN 層和一個輸出層。

```python
import mlx.nn as nn

class TextPredictionModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(TextPredictionModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        output, hidden = self.rnn(x)
        output = self.fc(output)
        return output

# 設定模型參數
embedding_dim = 128
hidden_dim = 512
vocab_size = len(chars)

# 初始化模型
model = TextPredictionModel(vocab_size, embedding_dim, hidden_dim)
```

---

### **4. 訓練模型**

我們將使用交叉熵損失函數（適用於分類問題）和 Adam 優化器來訓練模型。

```python
import mlx.optim as optim
import mlx

# 設定損失函數與優化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 訓練循環
epochs = 10
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for i in range(len(input_batch)):
        input_seq = mlx.tensor(input_batch[i], dtype=mlx.float32)
        target_seq = mlx.tensor(target_batch[i], dtype=mlx.long)

        optimizer.zero_grad()

        # 前向傳播
        output = model(input_seq)
        
        # 計算損失
        loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))
        total_loss += loss.item()

        # 反向傳播與優化
        loss.backward()
        optimizer.step()

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(input_batch):.4f}")
```

---

### **5. 模型評估與文本生成**

訓練完成後，我們可以使用模型來生成文本。模型會基於一個初始字符序列來預測接下來的字符，並根據預測結果迭代生成新的文本。

```python
# 文本生成函數
def generate_text(model, start_string, length=500):
    model.eval()

    # 將初始字符序列轉換為數字序列
    input_sequence = [char_to_idx[char] for char in start_string]

    # 生成文本
    generated_text = start_string
    input_tensor = mlx.tensor(input_sequence, dtype=mlx.long).unsqueeze(0)

    for _ in range(length):
        with mlx.no_grad():
            output = model(input_tensor)
            predicted_idx = mlx.argmax(output[0, -1, :], dim=-1).item()

        predicted_char = idx_to_char[predicted_idx]
        generated_text += predicted_char

        input_tensor = mlx.tensor([[predicted_idx]], dtype=mlx.long)

    return generated_text

# 使用模型生成文本
start_string = "shall i compare thee to a summer's day"
generated_text = generate_text(model, start_string)
print(generated_text)
```

---

### **6. 小結**

在這個案例中，我們展示了如何使用 MLX 構建一個簡單的字符級文本預測模型。這個模型基於 RNN 進行訓練，並能夠在給定初始文本的情況下生成新的文本。這樣的文本預測任務可以擴展到許多更復雜的語言模型和生成模型中，如基於 Transformer 的模型。