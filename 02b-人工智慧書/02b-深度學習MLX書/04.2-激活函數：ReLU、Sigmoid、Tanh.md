### **激活函數：ReLU、Sigmoid、Tanh**

激活函數在神經網路中扮演著至關重要的角色，它們是神經元中進行非線性變換的部分，使得神經網路能夠學習和擬合複雜的數據模式。激活函數的作用是將每個神經元的加權和（或輸入信號）轉換成輸出信號，這些輸出信號將進一步傳遞到下一層或作為最終輸出。

在這一節，我們將介紹三種常見的激活函數：ReLU、Sigmoid 和 Tanh，並探討它們的數學公式、特點及適用場景。

---

## **1. ReLU（Rectified Linear Unit）**

ReLU 是目前最為流行的激活函數之一，尤其在深度學習模型中。它的數學公式非常簡單：

\[
f(x) = \max(0, x)
\]

### **ReLU 的特點**：
- **非線性**：雖然 ReLU 是一個分段函數，但它仍然是非線性的。這使得神經網路能夠學習到複雜的模式。
- **簡單且計算高效**：ReLU 函數的計算過程只需要進行一次簡單的比較操作，因此它的計算速度非常快。
- **解決梯度消失問題**：與 Sigmoid 或 Tanh 等激活函數相比，ReLU 在正值範圍內不會飽和，因此有助於解決深度神經網路中的梯度消失問題。

### **ReLU 的缺點**：
- **死神經元問題**：當 ReLU 輸入值小於 0 時，其輸出始終為 0，這可能會導致神經元在訓練過程中“死亡”，即無法對輸入進行有效的響應。
  
### **適用場景**：
ReLU 是目前深度學習中最常用的激活函數，尤其是在卷積神經網路（CNN）和多層感知器（MLP）中，適用於大多數情況。

---

## **2. Sigmoid**

Sigmoid 激活函數是一種 S 形曲線，通常用於二元分類問題中。它的數學公式為：

\[
f(x) = \frac{1}{1 + e^{-x}}
\]

### **Sigmoid 的特點**：
- **範圍**：Sigmoid 函數的輸出範圍是 (0, 1)，這使得它特別適合用於概率預測，並且經常用於二分類問題中。
- **光滑且連續**：Sigmoid 函數是光滑的，這有助於神經網路進行平滑的梯度下降。
- **梯度消失問題**：當輸入值極大或極小時，Sigmoid 函數的梯度會非常小，這可能會導致梯度消失，從而減慢訓練過程，特別是在深層網路中。

### **Sigmoid 的缺點**：
- **梯度消失**：當輸入值過大或過小時，Sigmoid 函數的梯度非常小，這會導致梯度消失問題，從而影響神經網路的學習。
- **不以零為中心**：Sigmoid 函數的輸出範圍是 (0, 1)，這使得神經元的輸出總是為正數，這可能會使訓練過程中的梯度更新速度變慢。

### **適用場景**：
Sigmoid 激活函數在二元分類問題（例如邏輯回歸）中非常常見，尤其是當模型需要輸出概率時。此外，它也可以在較小的神經網路中使用。

---

## **3. Tanh（Hyperbolic Tangent）**

Tanh 函數是一個平滑的 S 形曲線，它是 Sigmoid 函數的擴展，輸出範圍為 (-1, 1)。其數學公式為：

\[
f(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
\]

### **Tanh 的特點**：
- **範圍**：Tanh 函數的輸出範圍是 (-1, 1)，這使得它在某些情況下比 Sigmoid 更為有用，因為其輸出的範圍更廣。
- **零為中心**：與 Sigmoid 不同，Tanh 函數的輸出範圍是對稱的，這樣可以加速神經網路的收斂過程，因為它對零對稱，對梯度下降有一定的促進作用。
- **梯度消失問題**：與 Sigmoid 類似，Tanh 函數在輸入值極大或極小時也會遇到梯度消失的問題。

### **Tanh 的缺點**：
- **梯度消失**：與 Sigmoid 一樣，Tanh 函數也會遇到梯度消失問題，尤其是在深層神經網路中，當輸入值很大或很小時，梯度會變得非常小。

### **適用場景**：
Tanh 函數通常比 Sigmoid 更受歡迎，特別是在需要零為中心的情況下。它在循環神經網路（RNN）中也有廣泛應用，尤其是當模型需要擁有更強的表現能力時。

---

## **4. MLX 中的激活函數實現**

在 MLX 中，使用激活函數非常簡單。下面是如何使用 `mlx.nn.ReLU`、`mlx.nn.Sigmoid` 和 `mlx.nn.Tanh` 來構建一個簡單的神經網路的示例：

```python
import mlx.core as mx
import mlx.nn as nn

# 定義一個帶有不同激活函數的 MLP 模型
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(784, 128)  # 輸入層到隱藏層
        self.relu = nn.ReLU()           # 激活函數 ReLU
        self.fc2 = nn.Linear(128, 10)   # 隱藏層到輸出層
        self.softmax = nn.Softmax(dim=1) # 用於類別概率

    def forward(self, x):
        x = self.fc1(x)  # 第一層運算
        x = self.relu(x)  # 激活函數 ReLU
        x = self.fc2(x)   # 第二層運算
        x = self.softmax(x)  # 輸出類別概率
        return x

# 創建模型實例並進行運算
model = MLP()
input_data = mx.array([[0.5]*784])  # 模擬一個 784 維的輸入向量
output = model(input_data)
print("Model output:", output)
```

在這個示例中：
- 我們使用了 `nn.ReLU()` 作為激活函數，並在網路的每層之間進行應用。

---

## **小結**

激活函數是神經網路中非常重要的組成部分，決定了每個神經元如何響應其輸入信號。ReLU 是當前最為常用的激活函數之一，因其計算簡單且能有效避免梯度消失問題；Sigmoid 和 Tanh 函數則在某些情況下也有其特有的優勢，特別是在二分類任務中。理解每種激活函數的特性及其適用場景，有助於我們在設計神經網路時做出更合適的選擇。

在 MLX 中，這些激活函數都可以輕鬆實現，幫助我們建立和訓練深度神經網路模型。