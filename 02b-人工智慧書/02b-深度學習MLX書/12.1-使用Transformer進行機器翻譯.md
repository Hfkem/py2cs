### 使用 Transformer 進行機器翻譯

在這一部分，我們將介紹如何使用 Transformer 模型進行機器翻譯任務。機器翻譯的目標是將一個語言的文本（源語言）翻譯成另一個語言的文本（目標語言）。我們將使用一個簡單的示例，使用 Transformer 進行英語到法語的翻譯。

#### 1. **數據準備**

首先，我們需要準備訓練數據集。機器翻譯通常使用平行語料庫，即包含源語言和目標語言對應句子的數據集。這裡我們使用一個簡單的英法平行語料庫進行示範。

假設我們有以下訓練數據：

```
English Sentences    |  French Sentences
-----------------------------------------
Hello               |  Bonjour
How are you?        |  Comment ça va?
I am learning       |  J'apprends
```

在實際應用中，我們需要對數據進行預處理，例如分詞、建立詞彙表、轉換文本為數字等。

#### 2. **預處理文本**

為了進行翻譯，我們首先需要將文本轉換為模型可以理解的數字格式。這通常需要分詞和詞彙表。

```python
from sklearn.preprocessing import LabelEncoder
import numpy as np

# 假設我們的英法語料庫如下：
english_sentences = ["Hello", "How are you?", "I am learning"]
french_sentences = ["Bonjour", "Comment ça va?", "J'apprends"]

# 使用 LabelEncoder 將文本轉換為數字
encoder = LabelEncoder()
encoder.fit(english_sentences + french_sentences)

# 將句子轉換為數字序列
english_sequences = [encoder.transform([s]).tolist() for s in english_sentences]
french_sequences = [encoder.transform([s]).tolist() for s in french_sentences]

# 假設最大序列長度
max_len = 10
english_sequences = [seq + [0] * (max_len - len(seq)) for seq in english_sequences]
french_sequences = [seq + [0] * (max_len - len(seq)) for seq in french_sequences]

print("English Sequences:", english_sequences)
print("French Sequences:", french_sequences)
```

#### 3. **Transformer 模型**

這裡我們可以基於之前的 Transformer 模型進行訓練。我們將建立源語言（英語）和目標語言（法語）之間的映射。

```python
class Translator(mlx.nn.Module):
    def __init__(self, d_model, num_heads, ff_dim, num_encoder_layers, num_decoder_layers):
        super(Translator, self).__init__()
        self.encoder_layers = mlx.nn.ModuleList([EncoderLayer(d_model, num_heads, ff_dim) for _ in range(num_encoder_layers)])
        self.decoder_layers = mlx.nn.ModuleList([DecoderLayer(d_model, num_heads, ff_dim) for _ in range(num_decoder_layers)])
        self.dense = mlx.nn.Linear(d_model, len(encoder.classes_))  # 設置為詞彙表大小

    def forward(self, src, tgt):
        # 編碼器處理源語言
        for layer in self.encoder_layers:
            src = layer(src)

        # 解碼器處理目標語言
        for layer in self.decoder_layers:
            tgt = layer(tgt, src)  # 目標語言和編碼器輸出作為解碼器輸入

        # 預測翻譯結果
        output = self.dense(tgt)
        return output
```

#### 4. **模型訓練**

現在，我們將訓練模型，使用優化器和損失函數來最小化翻譯的誤差。這裡使用的損失函數通常是交叉熵損失。

```python
# 創建模型
model = Translator(d_model=512, num_heads=8, ff_dim=2048, num_encoder_layers=6, num_decoder_layers=6)

# 優化器和損失函數
optimizer = mlx.optim.Adam(model.parameters(), lr=0.001)
criterion = mlx.nn.CrossEntropyLoss()

# 假設我們有輸入（源語言）和目標（翻譯語言）對應
src = mlx.tensor(english_sequences, dtype=mlx.float32)
tgt = mlx.tensor(french_sequences, dtype=mlx.float32)

# 訓練循環
for epoch in range(100):
    optimizer.zero_grad()
    output = model(src, tgt)
    
    # 計算損失
    loss = criterion(output.view(-1, len(encoder.classes_)), tgt.view(-1))
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")
```

#### 5. **生成翻譯結果**

訓練完成後，我們可以用模型生成翻譯結果。這通常涉及到將模型輸出轉換為詞彙表中最接近的詞，並將其映射回對應的語句。

```python
def translate(input_seq):
    model.eval()
    with torch.no_grad():
        src = mlx.tensor(input_seq, dtype=mlx.float32)
        # 初始的目標語言序列是空的
        tgt = mlx.tensor([[0] * max_len], dtype=mlx.float32)
        
        output = model(src, tgt)
        # 根據最大概率選擇翻譯結果
        predicted_sequence = output.argmax(dim=-1)
        return encoder.inverse_transform(predicted_sequence.numpy().tolist())

# 使用訓練後的模型翻譯新的英文句子
test_sentence = ["I am learning"]
test_seq = encoder.transform(test_sentence).reshape(1, -1)
translated_sentence = translate(test_seq)
print("Translated:", translated_sentence)
```

#### 6. **總結**

這是一個使用 Transformer 模型進行簡單機器翻譯任務的完整流程。這個流程包括了數據預處理、模型構建、訓練和生成翻譯結果等步驟。在實際應用中，通常需要更大的語料庫、更複雜的模型設計和更長的訓練時間來達到更高的翻譯質量。

Transformer 模型的強大之處在於其基於自注意力機制，可以高效地捕捉長距離依賴關係，這使得它在各種自然語言處理任務中表現出色。