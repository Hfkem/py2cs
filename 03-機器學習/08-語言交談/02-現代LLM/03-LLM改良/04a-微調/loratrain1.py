# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jifFbH8fZaPO8st9Miw-AvrYLl2o4Ykl
"""



# 确保在Colab中运行此脚本

# 1. 安装必要的库
!pip install transformers datasets accelerate bitsandbytes

# 2. 导入所需的库
import os
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling

# 3. 设置Hugging Face token (请替换为您的实际token)
os.environ["HUGGINGFACE_TOKEN"] = "xxx"

# 4. 加载预训练模型和分词器
model_name = "google/gemma-2b"
tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.environ["HUGGINGFACE_TOKEN"])
model = AutoModelForCausalLM.from_pretrained(model_name, token=os.environ["HUGGINGFACE_TOKEN"])

# 5. 准备数据集
# 假设您已经上传了名为 'your_data.json' 的JSON文件到Colab
dataset = load_dataset('json', data_files='/content/alpaca_chinese_part_0.json')

# 6. 数据预处理函数
#def preprocess_function(examples):
#    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)
def preprocess_function(examples):
    combined_texts = [
        instruction + " " + inp + " " + output
        for instruction, inp, output in zip(examples["instruction"], examples["input"], examples["output"])
    ]
    return tokenizer(combined_texts, truncation=True, padding="max_length", max_length=512)



# 7. 对数据集应用预处理
tokenized_datasets = dataset.map(preprocess_function, batched=True)

# 8. 设置训练参数
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    save_steps=10_000,
    save_total_limit=2,
    fp16=True,  # 啟用 mixed precision
    gradient_accumulation_steps=4,
)

# 9. 初始化Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
)

# 10. 开始训练
trainer.train()

# 11. 保存微调后的模型到本地
model_dir = "./finetuned_gemma_2b_lora"
trainer.save_model(model_dir)

# 12. 壓縮並下載模型
!zip -r finetuned_gemma_2b_lora.zip {model_dir}

from google.colab import files
files.download("finetuned_gemma_2b_lora.zip")

# 确保在Colab中运行此脚本

# 1. 安装必要的库
!pip install transformers datasets accelerate bitsandbytes peft

# 2. 导入所需的库
import os
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model  # 引入 LoRA 庫

# 3. 设置 Hugging Face token (请替换为您的实际 token)
os.environ["HUGGINGFACE_TOKEN"] = "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"

# 4. 加载预训练模型和分词器
model_name = "google/gemma-2b"
tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.environ["HUGGINGFACE_TOKEN"])
model = AutoModelForCausalLM.from_pretrained(model_name, token=os.environ["HUGGINGFACE_TOKEN"])

# 5. 配置 LoRA
lora_config = LoraConfig(
    r=8,  # LoRA 的秩，越小顯存佔用越少，推薦值為 4 或 8
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # 只在部分模型層上應用 LoRA，減少內存占用
    lora_dropout=0.1
)

# 将模型与 LoRA 结合
model = get_peft_model(model, lora_config)

# 6. 准备数据集
# 假设您已经上传了名为 'your_data.json' 的JSON文件到Colab
dataset = load_dataset('json', data_files='/content/alpaca_chinese_part_0.json')

# 7. 数据预处理函数（保持原有逻辑）
def preprocess_function(examples):
    combined_texts = [
        instruction + " " + inp + " " + output
        for instruction, inp, output in zip(examples["instruction"], examples["input"], examples["output"])
    ]
    return tokenizer(combined_texts, truncation=True, padding="max_length", max_length=512)

# 8. 对数据集应用预处理
tokenized_datasets = dataset.map(preprocess_function, batched=True)

# 9. 设置训练参数
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=1,  # 保持小批次大小以減少內存需求
    save_steps=10_000,
    save_total_limit=2,
    fp16=True,  # 啟用混合精度訓練，減少內存占用
    gradient_accumulation_steps=4,  # 使用梯度累加來模擬更大批次
)

# 10. 初始化 Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
)

# 11. 开始训练
trainer.train()
# 12. 保存微调后的模型到本地
model_dir = "./finetuned_gemma_2b_lora"
trainer.save_model(model_dir)

# 13. 壓縮並下載模型
!zip -r finetuned_gemma_2b_lora.zip {model_dir}

from google.colab import files
files.download("finetuned_gemma_2b_lora.zip")

# 确保在Colab中运行此脚本

# 1. 安装必要的库
!pip install transformers datasets accelerate bitsandbytes peft

# 2. 导入所需的库
import os
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model  # 引入 LoRA 庫

# 3. 设置 Hugging Face token (请替换为您的实际 token)
os.environ["HUGGINGFACE_TOKEN"] = "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"

# 4. 加载预训练模型和分词器
model_name = "google/gemma-2b"
tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.environ["HUGGINGFACE_TOKEN"])
model = AutoModelForCausalLM.from_pretrained(model_name, token=os.environ["HUGGINGFACE_TOKEN"])

# 5. 配置 LoRA
lora_config = LoraConfig(
    r=8,  # LoRA 的秩，越小顯存佔用越少，推薦值為 4 或 8
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # 只在部分模型層上應用 LoRA，減少內存占用
    lora_dropout=0.1
)

# 将模型与 LoRA 结合
model = get_peft_model(model, lora_config)

# 6. 准备数据集
# 假设您已经上传了名为 'your_data.json' 的JSON文件到Colab
dataset = load_dataset('json', data_files='/content/alpaca_chinese_part_0.json')

# 7. 数据预处理函数（保持原有逻辑）
def preprocess_function(examples):
    combined_texts = [
        instruction + " " + inp + " " + output
        for instruction, inp, output in zip(examples["instruction"], examples["input"], examples["output"])
    ]
    return tokenizer(combined_texts, truncation=True, padding="max_length", max_length=512)

# 8. 对数据集应用预处理
tokenized_datasets = dataset.map(preprocess_function, batched=True)

# 9. 设置训练参数
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=1,  # 保持小批次大小以減少內存需求
    save_steps=10_000,
    save_total_limit=2,
    fp16=True,  # 啟用混合精度訓練，減少內存占用
    gradient_accumulation_steps=4,  # 使用梯度累加來模擬更大批次
    remove_unused_columns=False  # 不移除數據集中未使用的欄位
)

# 10. 初始化 Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
)

# 11. 开始训练
trainer.train()

# 12. 保存微调后的模型到本地
model_dir = "./finetuned_gemma_2b_lora"
trainer.save_model(model_dir)

# 13. 壓縮並下載模型
!zip -r finetuned_gemma_2b_lora.zip {model_dir}

from google.colab import files
files.download("finetuned_gemma_2b_lora.zip")



