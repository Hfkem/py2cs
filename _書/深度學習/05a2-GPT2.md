
[The Journey of Open AI GPT models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb)

* GPT-2 had 48 layers and used 1600 dimensional vectors for word embedding.
* Larger vocabulary of 50,257 tokens was used.
* Larger batch size of 512 and larger context window of 1024 tokens were used.
* Layer normalisation was moved to input of each sub-block and an additional layer normalisation was added after final self-attention block.
* At initialisation, the weight of residual layers was scaled by 1/âˆšN, where N was the number of residual layers.

The authors trained four language models with 117M (same as GPT-1), 345M, 762M and 1.5B (GPT-2) parameters. Each subsequent model had lower perplexity than previous one. 

