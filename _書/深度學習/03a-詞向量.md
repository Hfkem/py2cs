

* [Vectors In Transformer Neural Networks](https://www.youtube.com/watch?v=l4is4uHvKlU)
    * 影片中提到難以理解為何 Positional Encoding 用加法加入詞向量，而不是獨立用一個額外的 Position 去表達呢？(只為了省空間?) (感覺應該用實驗來驗證哪一種比較好)
    * 但是 multi head 則用 concatanation ，花了很多倍的空間。
    * ccc: 單一個量+Positional Encoding ，和幾百維的量+Positional Encoding (一起平移) 是不同的，如果有平移不變性的話，意義是不會變太多的。
