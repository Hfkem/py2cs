
# 資訊理論

熵，交叉熵，KL 散度之間的關係

    cross_entropy(p,q) = entropy(p) + kl_divergence(p,q)

請看 ChatGPT 的解釋與 crossEntropy1.py

* https://chatgpt.com/c/67171e7a-4d6c-8012-9f5d-11bf7be050f8
    * https://chatgpt.com/share/67171ee0-3080-8012-be3d-6696ca14eebe

