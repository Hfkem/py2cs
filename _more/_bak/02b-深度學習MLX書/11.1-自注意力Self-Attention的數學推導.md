### 自注意力（Self-Attention）的數學推導

自注意力（Self-Attention）是許多現代深度學習模型，尤其是 Transformer 架構中不可或缺的一部分。它使模型能夠在處理序列數據時捕捉不同位置之間的依賴關係。自注意力的核心思想是對序列中每一個元素計算它與其他元素的關聯性，從而將這些關聯信息整合進去。

在本節中，我們將詳細介紹自注意力機制的數學推導，並解釋它如何在 Transformer 模型中運作。

#### 1. **自注意力機制的基本概念**

假設我們有一個輸入序列 \(X = [x_1, x_2, \dots, x_n]\)，其中每個 \(x_i\) 是一個向量，通常是詞嵌入或其他特徵表示。自注意力的目的是根據序列中各個元素之間的關聯性來計算每個元素的新的表示。

自注意力機制會對每個元素計算一個權重向量，該向量描述了它與序列中其他元素的相對重要性。具體來說，對於序列中的每一個元素 \(x_i\)，我們計算它與其他所有元素的“注意力分數”（即權重），並將這些分數加權求和來獲得該元素的新的表示。

#### 2. **自注意力的數學推導**

自注意力的計算過程包括以下幾個步驟：

1. **查詢（Query）、鍵（Key）、值（Value）**：首先，對輸入序列 \(X\) 中的每個元素 \(x_i\)，我們分別學習三個向量表示：
   - 查詢向量 \(Q_i = W_Q x_i\)
   - 鍵向量 \(K_i = W_K x_i\)
   - 值向量 \(V_i = W_V x_i\)

   這些向量的維度通常與輸入向量的維度相同，並且 \(W_Q, W_K, W_V\) 是可以學習的權重矩陣。

2. **計算注意力分數**：對於序列中的每一對元素 \(x_i\) 和 \(x_j\)，計算它們之間的注意力分數。這是通過查詢向量 \(Q_i\) 和鍵向量 \(K_j\) 的點積來實現的：

   \[
   \text{score}(Q_i, K_j) = Q_i^\top K_j
   \]

   這個分數反映了元素 \(x_i\) 和 \(x_j\) 之間的相關性。為了保證數值穩定性，通常將該分數除以一個縮放因子 \(\sqrt{d_k}\)，其中 \(d_k\) 是鍵向量的維度：

   \[
   \text{score}(Q_i, K_j) = \frac{Q_i^\top K_j}{\sqrt{d_k}}
   \]

3. **計算注意力權重**：為了將這些分數轉換為權重，我們使用 softmax 函數。對於每個查詢 \(Q_i\)，計算它與所有鍵的 softmax 值，這樣每個權重都是正數，且所有權重加起來為 1：

   \[
   \alpha_{ij} = \frac{\exp(\text{score}(Q_i, K_j))}{\sum_{k=1}^n \exp(\text{score}(Q_i, K_k))}
   \]

   這裡的 \(\alpha_{ij}\) 表示查詢 \(x_i\) 和鍵 \(x_j\) 之間的注意力權重。

4. **加權求和**：最後，對每個元素 \(x_i\)，我們根據注意力權重加權求和所有的值向量 \(V_j\) 來獲得新的表示：

   \[
   z_i = \sum_{j=1}^n \alpha_{ij} V_j
   \]

   這樣，通過將每個元素 \(x_i\) 的值向量加權求和，我們得到了更新的表示 \(z_i\)。

#### 3. **自注意力的最終表示**

經過上述步驟後，對於序列中的每個元素 \(x_i\)，我們都會得到一個新的表示 \(z_i\)。這些新的表示構成了自注意力層的輸出 \(Z\)，並可以用來進行後續的處理。

自注意力的公式可以總結為：

\[
Z = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\]

其中：
- \(Q\) 是查詢矩陣，包含所有查詢向量 \(Q_1, Q_2, \dots, Q_n\)
- \(K\) 是鍵矩陣，包含所有鍵向量 \(K_1, K_2, \dots, K_n\)
- \(V\) 是值矩陣，包含所有值向量 \(V_1, V_2, \dots, V_n\)
- \(\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)\) 計算所有查詢與鍵之間的相似度，並將其轉換為權重

#### 4. **多頭自注意力（Multi-Head Self-Attention）**

為了讓模型能夠學習到不同的注意力模式，Transformer 中引入了多頭自注意力機制。具體來說，我們將查詢、鍵和值向量分成多個頭（head），並在每個頭上分別計算自注意力，然後將所有頭的結果拼接在一起。這樣可以捕捉到不同的上下文信息。

數學上，多頭自注意力的計算過程可以表示為：

\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W_O
\]

其中每個頭的計算為：

\[
\text{head}_i = \text{softmax}\left(\frac{QW_Q^i (KW_K^i)^\top}{\sqrt{d_k}}\right) VW_V^i
\]

這樣，通過多頭自注意力，模型可以學習到更豐富的表示，進一步提高性能。

#### 5. **總結**

自注意力機制通過計算序列中每一個元素與其他元素之間的關聯性，並加權求和來生成新的表示。這使得模型可以根據序列中任意兩個位置的元素之間的關係來調整每個元素的表示。這一機制對於處理長序列的數據，尤其是文本數據，具有極大的優勢。自注意力的數學推導讓我們清楚地理解了如何通過查詢、鍵和值來捕捉序列中的關聯性。