### **神經元與多層感知器（MLP）結構**

在深度學習中，神經網路是由神經元（Neuron）和各種層（Layer）組成的，其中最基礎的結構就是多層感知器（MLP，Multi-Layer Perceptron）。MLP 是一種前向傳播的神經網路，它由多層全連接層（Fully Connected Layers，FC）構成，每一層的神經元都與前一層的每一個神經元相連。這些網路層在多數神經網路中用來進行分類、回歸和其他數據處理任務。

本節將深入介紹神經元和 MLP 結構，並展示如何在 MLX 框架中實現這些結構。

---

## **1. 神經元的基本結構**

一個神經元（或單一層）是神經網路的基本單位。它接收多個輸入，並通過加權和（weighted sum）進行計算，再經過激活函數進行非線性變換。

### **神經元的數學表示**：
對於一個簡單的單層神經元，其數學公式可以表示為：

\[
y = f\left( \sum_{i=1}^{n} w_i x_i + b \right)
\]

- \(x_i\)：輸入向量中的第 \(i\) 個元素
- \(w_i\)：對應的權重
- \(b\)：偏置項
- \(f\)：激活函數（如 Sigmoid、ReLU 等）
- \(y\)：神經元的輸出

神經元的運算過程如下：

1. **加權和**：對每一個輸入 \(x_i\)，與其對應的權重 \(w_i\) 相乘，再加上偏置項 \(b\)。
2. **激活函數**：將加權和的結果通過激活函數進行非線性變換，得到神經元的輸出 \(y\)。

常見的激活函數有：
- **Sigmoid**：適用於輸出值範圍在 [0, 1] 之間的情況。
- **ReLU（Rectified Linear Unit）**：適用於大多數深度學習模型，將所有負值設置為零。
- **Tanh**：範圍在 [-1, 1] 之間。

---

## **2. 多層感知器（MLP）結構**

多層感知器是一種前向神經網路結構，包含至少三層：
1. **輸入層**（Input Layer）：接收外部輸入數據，並將其傳遞到第一個隱藏層。
2. **隱藏層**（Hidden Layer）：包含多個神經元，對輸入數據進行處理並提取特徵。MLP 可以有一層或多層隱藏層。
3. **輸出層**（Output Layer）：將隱藏層的結果轉換為最終輸出，通常是類別標籤（在分類任務中）或預測值（在回歸任務中）。

每層的神經元都與前一層的所有神經元相連，並且每條邊都有一個權重值。這樣的結構使得 MLP 能夠學習複雜的數據模式。

### **MLP 的工作原理**：
1. **前向傳播（Forward Propagation）**：
   - 輸入層接收原始數據，並將其傳遞給隱藏層。
   - 隱藏層計算每個神經元的輸出，並將結果傳遞到下一層。
   - 最終，輸出層根據最後一層的結果產生最終輸出。

2. **反向傳播（Backward Propagation）**：
   - 根據預測結果與真實標籤之間的誤差，計算損失。
   - 反向傳播通過計算每一層的梯度，更新每個權重和偏置，以最小化損失。

---

## **3. MLX 中實現 MLP**

在 MLX 中，我們可以輕鬆地使用 `mlx.nn.Module` 類來定義 MLP。這是一個自定義的神經網路類，支持多層的建立、前向傳播以及參數訓練。

### **示例：構建一個簡單的 MLP**

下面是如何使用 MLX 定義並訓練一個簡單的 MLP 模型來解決分類問題的示例。

```python
import mlx.core as mx
import mlx.nn as nn

# 定義 MLP 模型
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        # 定義第一層：一個全連接層
        self.fc1 = nn.Linear(784, 128)  # 輸入 784 維，輸出 128 維
        self.relu = nn.ReLU()           # 激活函數 ReLU
        self.fc2 = nn.Linear(128, 10)   # 輸入 128 維，輸出 10 維（對應 10 類分類）
    
    def forward(self, x):
        x = self.fc1(x)  # 第一層運算
        x = self.relu(x)  # 激活
        x = self.fc2(x)   # 第二層運算
        return x

# 創建 MLP 模型實例
model = MLP()

# 假設我們有一個 784 維的輸入數據，並進行前向傳播
input_data = mx.array([[0.5]*784])  # 假設輸入數據是長度為 784 的向量
output = model(input_data)
print("Model output:", output)
```

在這個示例中：
- 我們定義了一個簡單的 MLP，包含兩層全連接層 `fc1` 和 `fc2`。
- 第一層將 784 維的輸入映射到 128 維，並應用了 ReLU 激活函數。
- 第二層將 128 維的輸出映射到 10 維，對應於 10 類分類的輸出。

---

## **4. 小結**

神經元是神經網路的基本單位，每個神經元根據其輸入的加權和計算輸出。多層感知器（MLP）是由多層神經元組成的神經網路結構，適用於多種機器學習任務，如分類和回歸。MLP 的結構通過前向傳播和反向傳播來進行學習，並且可以輕鬆擴展到多層網路，以捕捉更複雜的數據特徵。

在 MLX 中，使用 `nn.Module` 和 `nn.Linear` 等高級 API，可以方便地實現 MLP 網路結構，並進行高效訓練。接下來的章節將介紹 **激活函數與損失函數**，進一步深入了解神經網路模型的運作原理！