### **A2-機器學習數學公式速查表**

本章節提供機器學習領域常見的數學公式速查表，涵蓋線性代數、微積分、統計學、優化算法等數學工具，這些工具在機器學習模型的理解與實現中扮演重要角色。

---

### **1. 向量與矩陣運算**

#### 1.1 向量運算

- **內積**（Dot Product）
  \[
  \mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^n a_i b_i
  \]
  - 內積是兩個向量的乘積，得到一個標量。

- **外積**（Cross Product，僅對於三維向量）
  \[
  \mathbf{a} \times \mathbf{b} = \begin{vmatrix} \hat{i} & \hat{j} & \hat{k} \\ a_1 & a_2 & a_3 \\ b_1 & b_2 & b_3 \end{vmatrix}
  \]
  - 外積是兩個三維向量的叉積，結果是一個向量。

#### 1.2 矩陣運算

- **矩陣乘法**（Matrix Multiplication）
  \[
  C = A \cdot B
  \]
  - 如果矩陣 \(A\) 是 \(m \times n\)，矩陣 \(B\) 是 \(n \times p\)，則矩陣 \(C\) 的形狀是 \(m \times p\)。

- **轉置**（Transpose）
  \[
  A^T = \left[\begin{array}{ccc} a_{11} & a_{12} & \cdots \\ a_{21} & a_{22} & \cdots \\ \vdots & \vdots & \ddots \end{array}\right] \rightarrow A^T = \left[\begin{array}{ccc} a_{11} & a_{21} & \cdots \\ a_{12} & a_{22} & \cdots \\ \vdots & \vdots & \ddots \end{array}\right]
  \]
  - 轉置操作是將矩陣的行和列互換。

---

### **2. 微積分與優化**

#### 2.1 偏微分與梯度

- **偏微分**（Partial Derivative）
  \[
  \frac{\partial f(x, y)}{\partial x} = \lim_{\Delta x \to 0} \frac{f(x+\Delta x, y) - f(x, y)}{\Delta x}
  \]
  - 偏微分用於計算一個多變量函數對其中一個變量的導數。

- **梯度**（Gradient）
  \[
  \nabla f(\mathbf{x}) = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right]^T
  \]
  - 梯度是向量，包含了函數對所有變量的偏微分。

#### 2.2 梯度下降法（Gradient Descent）

- **梯度下降更新規則**
  \[
  \theta_{t+1} = \theta_t - \eta \nabla_{\theta} J(\theta)
  \]
  - 其中，\(\theta\) 是模型參數，\(\eta\) 是學習率，\(J(\theta)\) 是損失函數。

#### 2.3 龐加萊自動微分

- **自動微分**（Autograd）
  - 自動微分是通過鏈式法則自動計算復雜函數的梯度。對於多層神經網絡，它利用反向傳播計算每一層的梯度。

---

### **3. 統計與機率**

#### 3.1 機率分佈

- **正態分佈**（Normal Distribution）
  \[
  f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
  \]
  - 其中，\(\mu\) 是均值，\(\sigma\) 是標準差。

- **二項分佈**（Binomial Distribution）
  \[
  P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}
  \]
  - 其中，\(n\) 是實驗次數，\(k\) 是成功次數，\(p\) 是成功的概率。

#### 3.2 最大似然估計（MLE）

- **最大似然估計**：選擇參數\(\theta\) 使得觀察數據的似然函數最大化。
  \[
  L(\theta | X) = \prod_{i=1}^{n} P(x_i | \theta)
  \]
  - 最大化似然函數來估計模型參數。

---

### **4. 神經網絡相關公式**

#### 4.1 前向傳播

- **線性變換**（Linear Transformation）
  \[
  z = W x + b
  \]
  - 其中，\(W\) 是權重矩陣，\(x\) 是輸入向量，\(b\) 是偏置。

- **激活函數**（Activation Function）
  - **Sigmoid**
    \[
    \sigma(x) = \frac{1}{1 + e^{-x}}
    \]
  - **ReLU**
    \[
    \text{ReLU}(x) = \max(0, x)
    \]
  - **Tanh**
    \[
    \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
    \]

#### 4.2 反向傳播（Backpropagation）

- **反向傳播梯度更新規則**
  \[
  \delta = \frac{\partial L}{\partial z} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z}
  \]
  - 這裡，\(L\) 是損失函數，\(y\) 是輸出，\(z\) 是激活前的輸出。

---

### **5. 損失函數**

#### 5.1 均方誤差（MSE）

\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]
- \(y_i\) 是真實標籤，\(\hat{y}_i\) 是模型預測。

#### 5.2 交叉熵（Cross-Entropy）

\[
\text{Cross-Entropy} = - \sum_{i=1}^n y_i \log(\hat{y}_i)
\]
- 這是用於分類問題的損失函數，\(y_i\) 是真實標籤的指示函數，\(\hat{y}_i\) 是預測的概率。

---

### **6. 優化算法**

#### 6.1 隨機梯度下降（SGD）

\[
\theta_{t+1} = \theta_t - \eta \nabla_{\theta} J(\theta)
\]
- 這是最基本的梯度下降法，更新規則基於每個樣本的梯度。

#### 6.2 Adam 優化算法

\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_{\theta} J(\theta)
\]
\[
v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_{\theta} J(\theta))^2
\]
\[
\hat{m_t} = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v_t} = \frac{v_t}{1 - \beta_2^t}
\]
\[
\theta_{t+1} = \theta_t - \eta \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}
\]
- 其中，\(\beta_1\) 和 \(\beta_2\) 是動量和加速參數，\(\epsilon\) 是一個小常數，防止除零錯誤。

---

### **結語**

這份數學公式速查表包含了機器學習領域的核心數學工具，對於理解和實現各類機器學習算法至關重要。這些公式幫助我們將抽象的理論轉化為具體的計算步驟，進而在實際中應用。