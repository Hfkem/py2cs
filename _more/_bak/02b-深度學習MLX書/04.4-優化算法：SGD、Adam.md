### **優化算法：SGD、Adam**

在機器學習和深度學習中，優化算法是用來調整模型參數（如權重和偏置）的算法，以使得損失函數最小化。不同的優化算法有不同的更新策略，對於訓練過程的收斂速度和精度有著重要的影響。兩種常見的優化算法是隨機梯度下降（SGD）和自適應矩估計（Adam）。

---

## **1. 隨機梯度下降（SGD, Stochastic Gradient Descent）**

隨機梯度下降（SGD）是最基本的優化算法之一。它通過每次使用單個樣本來計算梯度並更新模型參數，相較於傳統的梯度下降法（Batch Gradient Descent），SGD 的計算效率更高，並且能夠處理大規模數據集。

### **SGD 更新規則**：

\[
\theta = \theta - \eta \nabla_\theta J(\theta)
\]

其中：
- \( \theta \) 是模型參數（權重和偏置）。
- \( \eta \) 是學習率。
- \( \nabla_\theta J(\theta) \) 是損失函數的梯度。

### **SGD 的特點**：
- **計算效率高**：每次更新使用的是單個訓練樣本，計算量比全量梯度下降小，因此適合大規模數據集。
- **收斂不穩定**：由於每次更新使用的樣本是隨機的，這會導致更新過程的波動，使得收斂過程不如批量梯度下降平滑。
- **需要調整學習率**：SGD 的學習率可能需要精心調整，否則可能導致收斂過慢或錯過最小值。

### **適用場景**：
SGD 適用於大規模的數據集，尤其是當計算資源有限且需要快速更新參數時。常見於機器學習和深度學習模型的訓練。

### **MLX 實現範例**：

在 MLX 中，我們可以使用 `mlx.optim.SGD` 來實現隨機梯度下降優化算法。以下是如何使用 SGD 優化器的範例：

```python
import mlx.core as mx
import mlx.optim as optim
import mlx.nn as nn

# 假設模型
model = nn.Linear(10, 2)  # 一個簡單的線性模型，輸入維度為 10，輸出維度為 2

# 假設損失函數
loss_fn = nn.MSELoss()

# 假設資料
inputs = mx.array([[1.0] * 10])  # 輸入樣本
targets = mx.array([[0.0, 1.0]])  # 真實標籤

# 使用 SGD 優化器
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 訓練步驟
optimizer.zero_grad()  # 清除之前的梯度
outputs = model(inputs)  # 前向傳播
loss = loss_fn(outputs, targets)  # 計算損失
loss.backward()  # 反向傳播
optimizer.step()  # 更新參數

print("Loss:", loss)
```

---

## **2. 自適應矩估計（Adam, Adaptive Moment Estimation）**

Adam 是一種更為先進的優化算法，結合了動量和自適應學習率的思想。與傳統的 SGD 不同，Adam 根據梯度的一階矩（即均值）和二階矩（即方差）來動態調整每個參數的學習率。

Adam 更新規則包含兩個主要步驟：
- 計算一階矩（均值）和二階矩（方差）。
- 根據這些矩估計調整學習率，進行參數更新。

### **Adam 更新規則**：

\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
\]
\[
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
\]
\[
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}
\]
\[
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
\]
\[
\theta_t = \theta_{t-1} - \frac{\eta \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\]

其中：
- \( m_t \) 和 \( v_t \) 分別是梯度的均值和方差。
- \( g_t \) 是在時刻 \( t \) 的梯度。
- \( \beta_1 \) 和 \( \beta_2 \) 是超參數，通常設定為接近 1（例如 0.9 和 0.999）。
- \( \eta \) 是學習率。
- \( \epsilon \) 是一個小常數（通常設定為 \( 10^{-8} \)，用於避免除以零）。

### **Adam 的特點**：
- **自適應學習率**：Adam 根據每個參數的梯度動態調整學習率，有助於提高收斂速度。
- **穩定的收斂**：由於動量和自適應學習率的引入，Adam 在大多數情況下能夠提供穩定且快速的收斂。
- **適用於大規模數據**：Adam 可以處理稀疏數據和非常高維的問題，通常比 SGD 更加高效。

### **適用場景**：
Adam 是目前深度學習領域中最流行的優化算法之一，適用於各種模型，尤其在處理大規模數據和高維度問題時表現出色。

### **MLX 實現範例**：

在 MLX 中，我們可以使用 `mlx.optim.Adam` 來實現 Adam 優化器。以下是如何使用 Adam 優化器的範例：

```python
import mlx.core as mx
import mlx.optim as optim
import mlx.nn as nn

# 假設模型
model = nn.Linear(10, 2)

# 假設損失函數
loss_fn = nn.MSELoss()

# 假設資料
inputs = mx.array([[1.0] * 10])  # 輸入樣本
targets = mx.array([[0.0, 1.0]])  # 真實標籤

# 使用 Adam 優化器
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 訓練步驟
optimizer.zero_grad()  # 清除之前的梯度
outputs = model(inputs)  # 前向傳播
loss = loss_fn(outputs, targets)  # 計算損失
loss.backward()  # 反向傳播
optimizer.step()  # 更新參數

print("Loss:", loss)
```

---

## **小結**

- **隨機梯度下降（SGD）**：簡單且高效，適用於大規模數據集，但收斂過程可能不穩定，並且學習率的選擇至關重要。
- **自適應矩估計（Adam）**：結合了動量和自適應學習率的優化算法，適用於各類型的問題，特別是在高維度和稀疏數據情況下表現更好。

在 MLX 中，這兩種優化算法都可以方便地實現，並根據具體問題選擇合適的優化器以提高模型的訓練效率和精度。