### **前向傳播與反向傳播**

在神經網絡的訓練過程中，**前向傳播**和**反向傳播**是兩個至關重要的步驟。它們分別處理數據如何從輸入層傳遞到輸出層，以及如何根據損失函數來更新模型的權重。這些步驟在訓練神經網絡時進行迭代，從而使模型能夠學習如何最小化損失函數。

---

### **1. 前向傳播（Forward Propagation）**

**前向傳播**是指數據從輸入層經過各層（例如隱藏層）到達輸出層的過程。在這個過程中，網絡會計算每一層的輸出，並使用激活函數來引入非線性特徵。最終，神經網絡會計算出預測結果。

#### **1.1 前向傳播的步驟**
假設我們的神經網絡包含兩層：一層隱藏層和一層輸出層，並且每層都有對應的權重和偏差。

1. **計算加權和**：每一層的每個神經元會接收來自上一層的輸入，並進行加權求和。對於第 \(i\) 個神經元來說，這個加權和是：
   \[
   z_i = \sum_{j=1}^{n} w_{ij} \cdot x_j + b_i
   \]
   其中，\(w_{ij}\) 是權重，\(x_j\) 是上一層的輸入，\(b_i\) 是偏差，\(z_i\) 是加權和。

2. **激活函數**：加權和經過激活函數（如 ReLU、Sigmoid 等）處理，將其轉換為神經元的輸出：
   \[
   a_i = \sigma(z_i)
   \]
   其中，\(\sigma\) 是激活函數，\(a_i\) 是該神經元的激活輸出。

3. **逐層計算輸出**：前向傳播會從輸入層開始，經過每一層的計算，直到最後得到輸出層的預測結果。

#### **1.2 例子：簡單的神經網絡**

考慮一個簡單的單層感知器（MLP），其結構如下：
- 輸入層：兩個特徵
- 隱藏層：兩個神經元，使用 ReLU 激活
- 輸出層：一個神經元，使用 Sigmoid 激活（適用於二分類問題）

前向傳播的過程如下：

```python
import mlx.nn as nn
import mlx.core as mx

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)  # 兩個輸入，兩個隱藏神經元
        self.fc2 = nn.Linear(2, 1)  # 兩個隱藏神經元，1 個輸出
        self.relu = nn.ReLU()       # ReLU 激活函數
        self.sigmoid = nn.Sigmoid() # Sigmoid 激活函數（適用於二分類）

    def forward(self, x):
        x = self.relu(self.fc1(x))   # 第一層加權和 + 激活
        x = self.sigmoid(self.fc2(x)) # 第二層加權和 + 激活
        return x

# 創建模型
model = SimpleNN()

# 輸入數據
input_data = mx.array([[0.5, 0.2]])

# 前向傳播
output = model(input_data)
print(output)
```

在這個例子中，`fc1` 和 `fc2` 分別是兩層的線性層，前向傳播過程包括：
- 第一層的加權和計算並通過 ReLU 激活。
- 第二層的加權和計算並通過 Sigmoid 激活，得到最終的預測。

---

### **2. 反向傳播（Backward Propagation）**

**反向傳播**是神經網絡訓練過程中的一個關鍵步驟，其主要作用是通過計算損失函數對各層權重的偏導數來更新權重，使得模型的預測結果更接近真實值。反向傳播使用了**鏈式法則**來計算每一層的梯度，然後通過優化器（如 SGD、Adam）來更新權重。

#### **2.1 反向傳播的步驟**

1. **計算損失函數的梯度**：
   在反向傳播的開始，我們首先計算損失函數對預測結果（網絡輸出）的梯度。對於二分類問題，常用的損失函數是 **交叉熵損失**。如果 \(y\) 是真實標籤，\(\hat{y}\) 是模型的預測，交叉熵損失為：
   \[
   L = -y \log(\hat{y}) - (1 - y) \log(1 - \hat{y})
   \]
   損失函數的梯度會告訴我們預測與真實標籤之間的差距。

2. **計算每一層的梯度**：
   反向傳播使用鏈式法則來計算每一層的梯度，這樣我們就可以知道每個權重對最終損失的貢獻。例如，對於第二層（輸出層），我們計算損失函數對輸出層權重的偏導數：
   \[
   \frac{\partial L}{\partial w} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial w}
   \]
   然後，根據這些梯度更新每一層的權重。

3. **更新權重**：
   最後，我們使用梯度下降或其他優化算法來根據梯度更新權重，使得損失函數的值逐漸減小，從而使模型的預測越來越準確。

#### **2.2 反向傳播的例子**

在 MLX 框架中，反向傳播會自動進行，只需在訓練循環中調用 `loss.backward()` 方法來計算梯度，並使用 `optimizer.step()` 來更新權重。以下是訓練循環的示例：

```python
import mlx.nn as nn
import mlx.core as mx

# 定義模型、損失函數和優化器
model = SimpleNN()
loss_fn = nn.BCELoss()
optimizer = nn.optim.Adam(model.parameters(), lr=0.001)

# 假設有一些訓練數據
inputs = mx.array([[0.5, 0.2], [0.7, 0.1]])
targets = mx.array([[1], [0]])

# 訓練循環
for epoch in range(1000):
    optimizer.zero_grad()  # 清除上一輪的梯度
    outputs = model(inputs)  # 前向傳播
    loss = loss_fn(outputs, targets)  # 計算損失
    loss.backward()  # 反向傳播，計算梯度
    optimizer.step()  # 更新權重

    # 每 100 次迭代輸出一次損失
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")
```

在這個例子中：
- `loss.backward()` 計算每層的梯度。
- `optimizer.step()` 使用計算得到的梯度更新權重。

---

### **3. 小結**

前向傳播和反向傳播是神經網絡訓練的兩個基本過程。前向傳播將輸入數據轉換為預測結果，並計算每一層的激活值；反向傳播則計算損失函數的梯度，並根據這些梯度更新權重，從而逐步優化模型的表現。在使用 MLX 等框架時，這些過程通常由框架自動處理，開發者只需關注模型設計、損失計算和優化過程。