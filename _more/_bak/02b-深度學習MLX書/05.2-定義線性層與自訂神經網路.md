### **定義線性層與自訂神經網路**

在 MLX 框架中，定義線性層（全連接層）和自訂神經網路模型非常簡單。`mlx.nn` 提供了多種層級結構，讓開發者可以根據需求設計網路結構。下面將介紹如何在 MLX 中定義線性層，並基於此構建一個自訂神經網絡。

---

### **1. 定義線性層**

線性層（Fully Connected Layer）是神經網絡中最常見的層，通過對輸入進行加權求和並加上偏置來進行輸出。在 MLX 中，線性層由 `mlx.nn.Linear` 類來定義。

#### **1.1 定義 `nn.Linear` 層**

`nn.Linear` 用來創建線性變換層，參數包括：
- `in_features`: 輸入特徵的數量（輸入維度）。
- `out_features`: 輸出特徵的數量（輸出維度）。

```python
import mlx.nn as nn

# 定義一個線性層，將輸入大小 10 轉換為 5
linear_layer = nn.Linear(in_features=10, out_features=5)
```

在這個例子中，`linear_layer` 會將大小為 `10` 的向量轉換為大小為 `5` 的向量。

#### **1.2 使用線性層**

為了進行計算，我們需要將一個輸入向量傳遞給這個線性層：

```python
import mlx.core as mx

# 創建一個包含 10 個特徵的輸入張量
inputs = mx.array([[1.0] * 10])

# 傳遞進線性層
outputs = linear_layer(inputs)
print(outputs)
```

這樣會返回一個大小為 `5` 的向量，這就是線性變換的結果。

---

### **2. 定義自訂神經網路**

在 MLX 中，我們可以通過繼承 `nn.Module` 來自訂神經網絡結構，並在 `forward` 函數中定義前向傳播的邏輯。這樣的設計使得神經網絡的結構可以非常靈活，並且支持各種層的組合。

#### **2.1 定義自訂神經網路模型**

以下是一個簡單的神經網絡模型，它包含兩個線性層，並使用 ReLU 激活函數：

```python
import mlx.nn as nn
import mlx.core as mx

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        # 定義第一層：線性層 (10 -> 50)
        self.fc1 = nn.Linear(10, 50)
        # 定義第二層：線性層 (50 -> 1)
        self.fc2 = nn.Linear(50, 1)
        # 定義激活函數：ReLU
        self.relu = nn.ReLU()

    def forward(self, x):
        # 前向傳播：通過第一層 -> 激活 -> 通過第二層
        x = self.relu(self.fc1(x))  # ReLU 激活
        x = self.fc2(x)  # 最終輸出
        return x

# 創建模型實例
model = SimpleNN()
```

在這個例子中，模型有兩層：
- 第一層 `fc1`：將輸入的 `10` 維度轉換為 `50` 維度。
- 第二層 `fc2`：將 `50` 維度轉換為 `1` 維度。

`forward` 方法定義了前向傳播過程：
1. 將輸入傳遞給第一層線性層 `fc1`，然後通過 ReLU 激活函數進行非線性變換。
2. 再將激活值傳遞給第二層線性層 `fc2`，得到最終輸出。

#### **2.2 創建並訓練模型**

模型創建完成後，可以進行訓練。下面是如何使用一些隨機數據進行簡單的訓練：

```python
# 假設的訓練數據
inputs = mx.array([[1.0] * 10])  # 10 維度的輸入
targets = mx.array([[1.0]])      # 目標標籤

# 定義損失函數與優化器
loss_fn = nn.MSELoss()  # 均方誤差損失
optimizer = nn.optim.Adam(model.parameters(), lr=0.001)

# 訓練步驟
optimizer.zero_grad()  # 清除之前的梯度
outputs = model(inputs)  # 前向傳播
loss = loss_fn(outputs, targets)  # 計算損失
loss.backward()  # 反向傳播
optimizer.step()  # 更新參數

print("訓練損失:", loss)
```

在這裡，`inputs` 是大小為 `10` 的輸入向量，`targets` 是大小為 `1` 的標籤。損失函數使用均方誤差（MSELoss），優化器選擇 Adam 優化器。通過前向傳播、損失計算、反向傳播和參數更新，我們完成了一次訓練步驟。

---

### **3. 小結**

- **線性層**：通過 `nn.Linear` 定義線性層，可以將高維度的輸入映射到低維度或高維度的輸出。這是神經網絡中的基本構建單元。
- **自訂神經網絡**：可以通過繼承 `nn.Module` 並覆寫 `forward` 方法來構建自訂神經網絡。這使得神經網絡的結構和計算過程非常靈活。
  
這種方式讓開發者能夠輕鬆地定義並訓練各種類型的神經網絡結構，包括多層感知器（MLP）、卷積神經網絡（CNN）等。