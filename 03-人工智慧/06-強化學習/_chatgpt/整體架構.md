# 強化學習整體架構

1. 有模型： 已知 P(s'|s,a)
    * 可以用策略改良，或者動態規劃找到最佳策略
2. 無模型： 未知 P(s'|s,a)
    * 增量法 (基於隨機近似理論中的 Robbins Monro 算法)
    * 問題是可能一直死掉，到不了終點，所以後面接近終點那些都測試不到。
    * 1. 此時可採用『起始探索』，讓任何 s0,a0 都能成為起始
    * 2. 無法採用『起始探索』的，可以用『柔性策略』（epsilon 柔性策略）（例如 epsilon 貪心就是給最好的 (1-epsilon), 剩下的 epsilon 平均分配給其他狀態。

## 無模型方法

1. Q-Learning: 

    Q[s,a] += alpha*(reward + gamma*np.max(Q[s1,:]) - Q[s,a])

2. SARSA: 

    a1 = np.argmax(Q[s1, :] + np.random.randn(1, env.action_space.n) * (1. / (i + 1)))  # 選擇下一步行動 a'
    Q[s, a] += alpha * (reward + gamma * Q[s1, a1] - Q[s, a])  # 使用 SARSA 更新公式

3. TD(lambda):


    a1 = np.argmax(Q[s1, :] + np.random.randn(1, env.action_space.n) * (1. / (i + 1)))
    delta = reward + gamma * Q[s1, a1] - Q[s, a]  # TD誤差計算

    # 更新資格跡 E[s,a]

    Q[s, a] += alpha * delta * E[s, a]

