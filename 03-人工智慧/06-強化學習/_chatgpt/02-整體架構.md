# 強化學習整體架構

1. 有模型： 已知 P(s'|s,a)
    * 可以用策略改良，或者動態規劃找到最佳策略
2. 無模型： 未知 P(s'|s,a)
    * 增量法 (基於隨機近似理論中的 Robbins Monro 算法)
    * 問題是可能一直死掉，到不了終點，所以後面接近終點那些都測試不到。
    * 1. 此時可採用『起始探索』，讓任何 s0,a0 都能成為起始
    * 2. 無法採用『起始探索』的，可以用『柔性策略』（epsilon 柔性策略）（例如 epsilon 貪心就是給最好的 (1-epsilon), 剩下的 epsilon 平均分配給其他狀態。

* [強化學習有模型和無模型，差異在哪？](https://chatgpt.com/c/6729a750-40bc-8012-afdf-004be2b6fc78)

## 無模型方法

1. Q-Learning: 

    $$
    Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
    $$

    Q[s,a] += alpha*(reward + gamma*np.max(Q[s1,:]) - Q[s,a])

2. SARSA: 

    $$
    Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma Q(s', a') - Q(s, a) \right)
    $$

    a1 = np.argmax(Q[s1, :] + np.random.randn(1, env.action_space.n) * (1. / (i + 1)))  # 選擇下一步行動 a'
    Q[s, a] += alpha * (reward + gamma * Q[s1, a1] - Q[s, a])  # 使用 SARSA 更新公式

3. TD(lambda):


    a1 = np.argmax(Q[s1, :] + np.random.randn(1, env.action_space.n) * (1. / (i + 1)))
    delta = reward + gamma * Q[s1, a1] - Q[s, a]  # TD誤差計算

    # 更新資格跡 E[s,a]

    Q[s, a] += alpha * delta * E[s, a]


## ccc: 何謂策略梯度定理？

策略梯度定理（Policy Gradient Theorem）是強化學習中的一個重要理論，主要用於優化策略函數。在強化學習中，策略（Policy）決定了智能體在給定環境狀態下該如何行動，而策略梯度方法則通過直接對策略進行微調來尋求最優策略。

**策略梯度定理的基本概念：**

假設有一個強化學習的環境，並且智能體的策略用 \( \pi_{\theta}(a|s) \) 來表示，其中 \( \theta \) 是策略的參數，\( a \) 是行動，\( s \) 是狀態。目標是最大化期望回報（或獎勳），這可以表示為：

\[
J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]
\]

其中：
- \( J(\theta) \) 是期望回報函數；
- \( r_t \) 是在時間步 \( t \) 的獎勳；
- \( \gamma \) 是折扣因子；
- \( \mathbb{E}_{\pi_{\theta}} \) 表示根據策略 \( \pi_{\theta} \) 進行的期望。

**策略梯度定理** 描述了如何計算這個期望回報對策略參數 \( \theta \) 的梯度。具體來說，策略梯度定理告訴我們，策略參數的梯度可以表示為：

\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \nabla_\theta \log \pi_{\theta}(a|s) Q^\pi(s, a) \right]
\]

其中：
- \( \nabla_\theta J(\theta) \) 是期望回報對策略參數 \( \theta \) 的梯度；
- \( \pi_{\theta}(a|s) \) 是在狀態 \( s \) 下採取行動 \( a \) 的概率；
- \( Q^\pi(s, a) \) 是在狀態 \( s \) 和行動 \( a \) 下的 Q 函數，表示從該狀態行動開始，後續的期望回報。

這個公式的意思是，策略的參數應該朝著增加 \( \log \pi_{\theta}(a|s) \) 和 \( Q^\pi(s, a) \) 的乘積的方向進行調整。簡單來說，這是一種基於策略的優化方法，其中智能體通過調整自己的策略參數來最大化期望回報。

**如何使用策略梯度：**

1. **採樣數據：** 透過與環境互動來收集狀態、行動和回報的序列。
2. **計算梯度：** 根據策略梯度定理計算梯度，這需要使用 \( \log \pi_{\theta}(a|s) \) 和 \( Q^\pi(s, a) \)。
3. **更新策略：** 使用梯度下降方法更新策略參數 \( \theta \)。

策略梯度方法適用於具有高維或連續動作空間的情況，特別是在難以直接建構值函數的情形下（例如策略基方法比值基方法更合適的場合）。