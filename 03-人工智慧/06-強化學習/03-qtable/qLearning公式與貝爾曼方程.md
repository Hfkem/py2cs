## 貝爾曼方程與 qLearning 表格更新公式

## 貝爾曼方程
貝爾曼方程（Bellman equation）是用來描述強化學習問題的一個重要公式，其形式是一個遞歸的方程，表示狀態值函數（state-value function）之間的關係。貝爾曼方程的基本形式如下：

$$V(s) = \max_{a}\{\sum_{s',r}p(s',r|s,a)[r + \gamma V(s')]\}$$

其中，$V(s)$ 表示狀態 $s$ 的值，$a$ 是可以進行的操作，$p(s',r|s,a)$ 表示狀態轉移的概率和奖赏概率，$s'$ 表示轉移後的狀態，$r$ 表示获得的奖赏，$\gamma \in [0, 1]$ 是折扣因子，表示未来奖赏的价值。

貝爾曼方程最主要的作用是可以用来计算某个状态的价值，而且这个状态的价值是由它的后继状态的价值组成的，而后继状态的价值又由它更下一层的后继状态的价值组成，以此类推，最终回溯到一些确定的状态，这些状态的价值是我们已知的，如评估状态价值或者评估动作-状态价值等强化学习问题中的常见问题。

## Q-Learning

Q-Learning是一种典型的无模型（model-free）的强化学习算法，它通过迭代地更新动作值函数来逐渐逼近最优动作值函数，从而实现最优的决策。Q-Learning常常表达为以下的形式：

$$Q(s_t, a_t)←Q(s_t, a_t)+\alpha(reward+\gamma \max_{a} Q(s_{t+1},a)-Q(s_t,a_t))$$

其中，$Q(s_t,a_t)$ 表示在状态 $s_t$ 时选择动作 $a_t$ 所得的收益值，$\alpha$ 是学习率（learning rate），$reward$ 是奖赏（reward），$\gamma$ 是折扣因子。$\max_{a}Q(s_{t+1},a)$ 表示选择在下一个状态 $s_{t+1}$ 时能够得到的最大收益值。

Q-Learning的关键在于更新动作值函数，通过逐渐迭代动作值函数来逼近最优动作值函数，最终学习到最优的策略。可能的动作数量是有限的，因此可以维护一个Q表的数据结构，它在每个状态下保存每个可能动作的预期回報值。随着学习的进行，Q表的值将被不断更新，最终得到最优的预期回报值，从而使智能体通过动作值函数得到最优的动作并获得最大的实际奖励。